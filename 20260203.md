# 20260203

## stacking df
### 如何设置多层索引
#### `set_index`
![](https://cdn.jsdelivr.net/gh/twicesquare/pics@main/20260203122808934.png)
inplace设置是否是在原DF上更改
#### `pd.MultiIndex.from_arrays`
![](https://cdn.jsdelivr.net/gh/twicesquare/pics@main/20260203123110479.png)
行、列同时设置索引
#### `stack()`
意味着把**最里层的**列索引重新排列成最里层的行索引
![](https://cdn.jsdelivr.net/gh/twicesquare/pics@main/20260203123431784.png)
`level=0`可以选择最靠外层的列索引，而不是最里层，不设置level则默认最里层
![](https://cdn.jsdelivr.net/gh/twicesquare/pics@main/20260203123559295.png)
`level=year`按名字来索引

## unstacking df
### `unstack()`
将最里层的行索引转换成最里层的列索引
`unstack(level=0)`移动的是最外层的行索引，不设置就是默认最里层，以此类推
### 循环使用stack和unstack来对DF排列
![](https://cdn.jsdelivr.net/gh/twicesquare/pics@main/20260203124521911.png)

## 交换index级别
### `df.swaplevel(1,2,aixs=0/1)`
交换两个index,行索引/列索引

可以和stack或unstack结合使用
## 同时堆叠/拆分多个index
### `unstack(level=[0,1])`
顺序很重要，最后一个就是最里层，以此类推

## 在stack/unstack时处理丢失的数据
### `unstack(level=1,fill_value='自己随便填')`
### `stack(dropna=False).fillna(自己想要的)`
默认dropna是true，删除全部为NAN的行，这样有时候会损失信息

## 分组与统计
### 与计算结合
![](https://cdn.jsdelivr.net/gh/twicesquare/pics@main/20260203132535059.png)
1. `sales["office supply"]`

逻辑：从原始数据中筛选出商品类别为 office supply 的子集。

2. `.unstack(level='country')`

逻辑：将行索引（Index）中的 country 这一层级“解开”，推到列（Columns）上。

结果：你可以看到表格上方出现了 Italy 和 Spain。由于原始数据中还有 shop（online/onsite）这一层，所以现在列索引变成了多级索引（MultiIndex）。

3. `.diff(axis=1, periods=2) —— 核心计算`

这是最关键的一步，它在水平方向（列与列之间）做减法：

    axis=1：告诉 Pandas “横向计算”。
    
    periods=2：告诉 Pandas “跨过 2 列做减法”。
    
    为什么图中有 NaN？
    因为 periods=2 代表用 当前列 减去 往前数第2列：  
    第 1 列 (online/Italy)：前面没数据，结果为 NaN。  
    第 2 列 (online/Spain)：前面只有 1 列，不够 2 列，结果为 NaN。  
    第 3 列 (onsite/Italy)：它减去第 1 列 (online/Italy)，计算的是 “同一国家，不同店铺类型” 的差值。  
    第 4 列 (onsite/Spain)：它减去第 2 列 (online/Spain)。

### 与groupby结合
![](https://cdn.jsdelivr.net/gh/twicesquare/pics@main/20260203133245618.png)
`sales.groupby(level=1).median().stack(level=[0, 1]).unstack(level='year')`

groupby(level=1).median(): 按索引的第 2 层（通常是类别，如 Technology）分组并计算中位数。

.stack(level=[0, 1]): 这是一个“折叠”操作。它把原本水平排列的列标题（可能是 shop 类型和指标）全部堆叠到行索引里。

.unstack(level='year'): 这是一个“展开”操作。它把 year 从行里抽出来放在最顶端作为列名。

## 处理含有多个值的单列, list-like columns [, ,]
### df.explode()
![](https://cdn.jsdelivr.net/gh/twicesquare/pics@main/20260203134447178.png)
![](https://cdn.jsdelivr.net/gh/twicesquare/pics@main/20260203134525346.png)
更快点的方法
### 使用reset_index保证explode后的index的唯一性
![](https://cdn.jsdelivr.net/gh/twicesquare/pics@main/20260203134622103.png)
### 处理,分割的字符串
![](https://cdn.jsdelivr.net/gh/twicesquare/pics@main/20260203135005422.png)

## JSON
### json_normalize(xxx,record_path='xxx',meta=['',''])
`from pandas import json_normalize`

`json_normalize`展平json格式并转化为DF

`record_path`更详细的展平某个列（并只保留这列）

`meta`，在record_path之后，还需要带上其他哪些列

## 如何处理DF列内的嵌套数据(1)
`import json`
![](https://cdn.jsdelivr.net/gh/twicesquare/pics@main/20260203142209865.png)
`apply`使得任何函数能被应用

`apply(json.loads):`

原始数据 collection['books'] 每一行都是一个字符串（例如 '{"title": "Frankenstein", "year": 1818}'）。

这一步将字符串解析为 Python 的字典 (Dictionary) 对象。

`pd.series`

这是将字典转换成表格的“黑魔法”。

它将字典里的每一个 Key（如 title, year）变成 DataFrame 的列名，将对应的 Value 变成单元格内容。
### `birds = pd.DataFrame(dict(names=names, bird_facts=bird_facts))`
### `pd.concat`
![](https://cdn.jsdelivr.net/gh/twicesquare/pics@main/20260203142526368.png)

## 如何处理DF列内的嵌套数据(2)
### `dumps()`
![](https://cdn.jsdelivr.net/gh/twicesquare/pics@main/20260203142654557.png)
`apply(json.loads)`：首先将 books 列中每一行的 JSON 字符串解析为 Python 字典。

`.to_list()`：将解析后的所有字典转换成一个标准的 Python 列表（即列表中的每个元素都是一个字典）。

`json.dumps`：将上一步生成的字典列表重新打包，转化成一个单一的、巨大的 JSON 格式长字符串。这个字符串现在代表一个 JSON 数组。

`pd.read_json`：这是 Pandas 用于读取 JSON 数据的内置函数。它直接接收上一步生成的 JSON 字符串，并自动将其解析为 DataFrame。

**在上一张图中，使用的是 apply(pd.Series)。而本图展示的方法通常效率更高：性能更好：
当数据量很大时，apply(pd.Series) 会针对每一行创建一个 Series 对象再合并，速度较慢。而 pd.read_json 是底层优化过的批量操作，处理成千上万行数据时速度显著更快。
结构清晰：它利用了“列表 -> 字符串 -> 整体读取”的逻辑，避开了逐行操作的开销。**

### 一个方法1的例子
```
# Define birds reading names and bird_facts lists into names and bird_facts columns
birds = pd.DataFrame(dict(names=names, bird_facts=bird_facts))

# Apply to bird_facts column the function loads from json module
data_split = birds['bird_facts'].apply(json.loads).apply(pd.Series)

# Remove the bird_facts column from birds
birds = birds.drop(columns='bird_facts')

# Concatenate the columns of birds and data_split
birds = pd.concat([birds,data_split], axis=1)

# Print birds
print(birds)
```
### 方法2的例子
```
# Apply json.loads to the bird_facts column and transform it to a list 
birds_facts = birds['bird_facts'].apply(json.loads).to_list()

# Convert birds_fact into a JSON 
birds_dump = json.dumps(birds_facts)

# Read the JSON birds_dump into a DataFrame
birds_df = pd.read_json(birds_dump)

# Concatenate the 'names' column of birds with birds_df 
birds_final = pd.concat([birds['names'], birds_df], axis=1)

# Print birds_final
print(birds_final)
```
