# 20260123

### several useful methods
`.head()` returns the first few rows (the “head” of the DataFrame).  
`.info()` shows information on each of the columns, such as the data type and number of missing values.  
`.shape` returns the number of rows and columns of the DataFrame.  
`.describe()` calculates a few summary statistics for each column.  

### three components
`.values`: A two-dimensional NumPy array of values.  
`.columns`: An index of columns: the column names.  
`.index`: An index for the rows: either row numbers or row names.  

### Sorting rows
sort the rows by passing a column name to `.sort_values()`  
|Sort on … |	Syntax |
|:--- | :--- |
|one column |	df.sort_values("breed") |
|multiple columns |	df.sort_values(["breed", "weight_kg"]) |

By combining `.sort_values()` with `.head()`, you can answer questions in the form, "What are the top cases where…?".

### Subsetting columns
Square brackets (`[]`) can be used to select only the columns that matter to you in an order that makes sense to you.  
To select only "col_a" of the DataFrame df, use `df["col_a"]`  
To select "col_a" and "col_b" of df, use `df[["col_a", "col_b"]]`  

### Subsetting rows
return True or False for each row, then pass that inside square brackets.  
```
dogs[dogs["height_cm"] > 60]
dogs[dogs["color"] == "tan"]
```
filter for multiple conditions at once by using the "bitwise and" operator, &  
```
dogs[(dogs["height_cm"] > 60) & (dogs["color"] == "tan")]
```

### Subsetting rows by categorical variables
Subsetting data based on a categorical variable often involves using the or operator (|) to select rows from multiple categories.   
`.isin()` method, which will allow you to tackle this problem by writing one condition instead of three separate ones.
```
colors = ["brown", "black", "tan"]
condition = dogs["color"].isin(colors)
dogs[condition]
```

### combo1
```
# Create indiv_per_10k col as homeless individuals per 10k state pop
homelessness["indiv_per_10k"] = 10000 * homelessness["individuals"] / homelessness["state_pop"] 

# Subset rows for indiv_per_10k greater than 20
high_homelessness = homelessness[homelessness["indiv_per_10k"] > 20]

# Sort high_homelessness by descending indiv_per_10k
high_homelessness_srt = high_homelessness.sort_values("indiv_per_10k", ascending = False)

# From high_homelessness_srt, select the state and indiv_per_10k cols
result = high_homelessness_srt[["state", "indiv_per_10k"]]

# See the result
print(result)
```

### Summary statistics
```
# Print the mean of weekly_sales
print(sales["weekly_sales"].mean())

# Print the median of weekly_sales
print(sales["weekly_sales"].median())
```
![](https://cdn.jsdelivr.net/gh/twicesquare/pics@main/20260123201046826.png)
#### The .agg() method 
```
df['column'].agg(function)
```
```
# Create a custom IQR function
def iqr(column):
    return column.quantile(0.75) - column.quantile(0.25)

import numpy as np

# Update to print IQR and median of temperature_c, fuel_price_usd_per_l, & unemployment
print(sales[["temperature_c", "fuel_price_usd_per_l", "unemployment"]].agg([iqr, np.median]))
```
"IQR" is short for inter-quartile range, which is the 75th percentile minus the 25th percentile.   
![](https://cdn.jsdelivr.net/gh/twicesquare/pics@main/20260123200956768.png)
```
# Sort sales_1_1 by date
sales_1_1 = sales_1_1.sort_values("date")

# Get the cumulative sum of weekly_sales, add as cum_weekly_sales col
sales_1_1["cum_weekly_sales"] = sales_1_1["weekly_sales"].cumsum()

# Get the cumulative max of weekly_sales, add as cum_max_sales col
sales_1_1["cum_max_sales"] = sales_1_1["weekly_sales"].cummax()

# See the columns you calculated
print(sales_1_1[["date", "weekly_sales", "cum_weekly_sales", "cum_max_sales"]])
```

### Summarizing dates
Summary statistics can also be calculated on date columns that have values with the data type datetime64. Some summary statistics — like mean — don't make a ton of sense on dates, but others are super helpful, for example, minimum and maximum, which allow you to see what time range your data covers.
```
# Print the maximum of the date column
print(sales["date"].max())

# Print the minimum of the date column
print(sales["date"].min())
```

### Counting
subset 参数接受一个列表，只有这两列都相同的行才会被去重  
`sales.drop_duplicates(subset=["store", "type"])`  

1. 筛选 is_holiday 为 True 的行  
2. 对 "date" 列去重，确保每个节假日日期只出现一次  
`sales[sales["is_holiday"]].drop_duplicates(subset="date")`

`store_types["type"].value_counts()`  
`value_counts()` 有一个非常强大的参数 `normalize`  
如果你想知道每种类型的商店占总数的比例，可以这样做：  
```
# 设置 normalize=True，结果会是 0 到 1 之间的小数（比例）  
store_props = store_types["type"].value_counts(normalize=True)
```

`value_counts`的三个可用参数   
`store_depts["department"].value_counts(sort=True, ascending = False, normalize=True)`

### Grouped summary statistics
![](https://cdn.jsdelivr.net/gh/twicesquare/pics@main/20260123205759529.png)
![](https://cdn.jsdelivr.net/gh/twicesquare/pics@main/20260123205831985.png)
![](https://cdn.jsdelivr.net/gh/twicesquare/pics@main/20260123205911052.png)
![](https://cdn.jsdelivr.net/gh/twicesquare/pics@main/20260123205950559.png)



